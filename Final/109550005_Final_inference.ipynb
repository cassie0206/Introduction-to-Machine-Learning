{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!pip install feature_engine\n","!pip install tensorflow_addons"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lXrDtQFQXPz5","executionInfo":{"status":"ok","timestamp":1673066569738,"user_tz":-480,"elapsed":7046,"user":{"displayName":"張可晴","userId":"06130326083553074662"}},"outputId":"74267c03-7f69-4566-c32e-fe9d4a1982c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: feature_engine in /usr/local/lib/python3.8/dist-packages (1.5.2)\n","Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from feature_engine) (1.0.2)\n","Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.8/dist-packages (from feature_engine) (1.21.6)\n","Requirement already satisfied: pandas>=1.0.3 in /usr/local/lib/python3.8/dist-packages (from feature_engine) (1.3.5)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from feature_engine) (1.7.3)\n","Requirement already satisfied: statsmodels>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from feature_engine) (0.12.2)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.3->feature_engine) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.3->feature_engine) (2022.7)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->feature_engine) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->feature_engine) (3.1.0)\n","Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.8/dist-packages (from statsmodels>=0.11.1->feature_engine) (0.5.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from patsy>=0.5->statsmodels>=0.11.1->feature_engine) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.8/dist-packages (0.19.0)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow_addons) (2.7.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow_addons) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow_addons) (3.0.9)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DlrWsVeJziRz","executionInfo":{"status":"ok","timestamp":1673066571807,"user_tz":-480,"elapsed":2080,"user":{"displayName":"張可晴","userId":"06130326083553074662"}},"outputId":"b9016988-bcfd-4a91-f6ba-aabeea8be46b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["# for google colab\n","from google.colab import drive\n","# mount your Google Drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["# for google colab\n","# copy all files from \"HW5\" directory in Google drive to current directory\n","!cp -r ./gdrive/MyDrive/Final/* ."],"metadata":{"id":"lG_LL0PlzmFJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import sys\n","import joblib\n","import numpy as np\n","import pandas as pd\n","import gc; gc.enable()\n","from lightgbm import LGBMClassifier\n","from sklearn.impute import KNNImputer\n","from sklearn.metrics import roc_auc_score\n","from sklearn.naive_bayes import GaussianNB\n","from feature_engine.encoding import WoEEncoder\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.linear_model import LogisticRegression, HuberRegressor\n","from keras.models import load_model\n","import warnings; warnings.filterwarnings(\"ignore\")\n","from tensorflow.keras import Sequential  \n","from tensorflow.keras import layers\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","\n","pd.options.display.max_columns = 999"],"metadata":{"id":"oZBqBpJsznky"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = pd.read_csv('train.csv')\n","test = pd.read_csv('test.csv')\n","submission = pd.read_csv('sample_submission.csv')"],"metadata":{"id":"MhE7A7lj2O0k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. fill the missing value (HuberRegressor + KNNImputer) 2. change attribute 0 to woe\n","def preprocessing(df_train, df_test):\n","  # dictionnary of dictionnaries (for the 11 best correlated measurement columns), \n","  # we will use the dictionnaries below to select the best correlated columns according to the product code)\n","  # Only for 'measurement_17' we make a 'manual' selection :\n","  full_fill_dict ={}\n","  full_fill_dict['measurement_17'] = {\n","      'A': ['measurement_5','measurement_6','measurement_8'],\n","      'B': ['measurement_4','measurement_5','measurement_7'],\n","      'C': ['measurement_5','measurement_7','measurement_8','measurement_9'],\n","      'D': ['measurement_5','measurement_6','measurement_7','measurement_8'],\n","      'E': ['measurement_4','measurement_5','measurement_6','measurement_8'],\n","      'F': ['measurement_4','measurement_5','measurement_6','measurement_7'],\n","      'G': ['measurement_4','measurement_6','measurement_8','measurement_9'],\n","      'H': ['measurement_4','measurement_5','measurement_7','measurement_8','measurement_9'],\n","      'I': ['measurement_3','measurement_7','measurement_8']\n","  }\n","\n","  # data = train + test => take both train and test data into consideration\n","  data = pd.concat([df_train, df_test]) \n","  # construct additional column to record the loss data for measurement_3 & measurement_5\n","  data['m3_missing'] = 1 * data['measurement_3'].isnull()\n","  data['m5_missing'] = 1 * data['measurement_5'].isnull()\n","  data['area'] = data['attribute_2'] * data['attribute_3']\n","\n","  # calculate the important order of all measurements which depends on correlation\n","  # filter out the column that has no relation to measurement ramaining the related one and keep them in corelated_data\n","  correlated_data = data[['measurement_' + str(i) for i in range(18)] + ['failure', 'area']]\n","  val = []\n","  col =[]\n","  for x in range(3,17):\n","    cor_val = correlated_data.corr()['measurement_' + str(x)] # data.corr()表示了data中的两个变量之间的相关性\n","    cor_val = np.absolute(cor_val)\n","    total_val = np.sum(cor_val.sort_values(ascending=False)[1:4]) # get most 3 correlated value\n","    val.append(np.round(total_val,3)) \n","    col.append('measurement_' + str(x))\n","\n","  c = pd.DataFrame()\n","  c['corelated columns'] = col\n","  c['correlated value'] = val\n","  c = c.sort_values(by='correlated value', ascending=False).reset_index(drop=True)\n","\n","  # we just pick the most important 10 measurements\n","  # find the best corelated columns based on the product code as the initial format of measurement17\n","  for i in range(10):\n","    measurement_col = 'measurement_' + c.iloc[i,0][12:] # we select the next best correlated column \n","    fill_dict = {}\n","    for x in data['product_code'].unique() : \n","      cor_val = correlated_data[data['product_code'] == x].corr()[measurement_col]\n","      cor_val = np.absolute(cor_val).sort_values(ascending=False)\n","      measurement_col_dic = {}\n","      measurement_col_dic[measurement_col] = cor_val[1:5].index.tolist() # keep the most important 4 measurement\n","      fill_dict[x] = measurement_col_dic[measurement_col]\n","    full_fill_dict[measurement_col] = fill_dict\n","\n","  # start running depends on product code\n","  for code in data['product_code'].unique():\n","    # use HuberRegressor to fill the missing value\n","    for measurement_col in list(full_fill_dict.keys()):\n","      # extract the current product code data\n","      tmp = data[data['product_code'] == code]\n","      # extract the correlated measurement we just claculated\n","      column = full_fill_dict[measurement_col][code]\n","      # collect all corelated measurement's data and drop rows which contain missing values\n","      tmp_train = tmp[column + [measurement_col]].dropna(how='any')\n","      # collect the data that doesn't miss data\n","      tmp_test = tmp[(tmp[column].isnull().sum(axis=1) == 0) & (tmp[measurement_col].isnull())]\n","      model = HuberRegressor(epsilon=1.9)\n","      model.fit(tmp_train[column], tmp_train[measurement_col])\n","      data.loc[(data['product_code'] == code) & (data[column].isnull().sum(axis=1) == 0) & (data[measurement_col].isnull()), measurement_col] = model.predict(tmp_test[column])\n","\n","    # use KNNImputer to fill the missing value\n","    nullValue_cols = [col for col in df_train.columns if df_train[col].isnull().any()] # keep the column with loss data\n","    # calculate the total missing data depends on each measurement and current product code\n","    NA = data.loc[data['product_code'] == code, nullValue_cols].isnull().sum().sum()\n","    # Imputation for completing missing values using k-Nearest Neighbors.\n","    model1 = KNNImputer(n_neighbors=3) \n","    feature = ['loading'] + ['measurement_' + str(i) for i in range(18)]\n","    data.loc[data['product_code'] == code, feature] = model1.fit_transform(data.loc[data['product_code'] == code, feature])\n","\n","  data['measurement_avg'] = data[['measurement_' + str(i) for i in range(3, 17)]].mean(axis=1)\n","\n","  # replaces categories by the weight of evidence\n","  df_train = data.iloc[:len(df_train),:]\n","  df_test = data.iloc[len(df_train):,:]\n","  woe_encoder = WoEEncoder(variables=['attribute_0'])\n","  woe_encoder.fit(df_train, df_train['failure'])\n","  df_test = woe_encoder.transform(df_test)\n","  \n","  return df_test"],"metadata":{"id":"PGo3xO0yzu26"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test = preprocessing(train, test)\n","features = ['loading', 'attribute_0', 'measurement_17', 'measurement_0', \n","      'measurement_1', 'measurement_2','measurement_3','measurement_4',\n","      'measurement_5', 'measurement_6','measurement_7','measurement_8',\n","      'measurement_9','measurement_10','measurement_11', 'measurement_12',\n","      'measurement_13','measurement_14','measurement_15','measurement_16', 'measurement_17',\n","      'area', 'm3_missing', 'm5_missing', 'measurement_avg']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GlMFsQhe0ue6","executionInfo":{"status":"ok","timestamp":1673066599240,"user_tz":-480,"elapsed":26009,"user":{"displayName":"張可晴","userId":"06130326083553074662"}},"outputId":"119f9d12-14e9-49d5-9406-75ff3f7d5706"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","-------- Product code A ----------\n","\n","filled by linear model :\n","measurement_17 : 386\n","measurement_8 : 167\n","measurement_11 : 225\n","measurement_5 : 113\n","measurement_6 : 146\n","measurement_7 : 153\n","measurement_4 : 79\n","measurement_15 : 273\n","measurement_10 : 209\n","measurement_16 : 293\n","measurement_14 : 237\n","\n","2281 filled by linear model \n","1568 filled by KNN \n","\n","-------- Product code B ----------\n","\n","filled by linear model :\n","measurement_17 : 418\n","measurement_8 : 165\n","measurement_11 : 220\n","measurement_5 : 83\n","measurement_6 : 106\n","measurement_7 : 176\n","measurement_4 : 80\n","measurement_15 : 294\n","measurement_10 : 197\n","measurement_16 : 358\n","measurement_14 : 330\n","\n","2427 filled by linear model \n","1548 filled by KNN \n","\n","-------- Product code C ----------\n","\n","filled by linear model :\n","measurement_17 : 391\n","measurement_8 : 211\n","measurement_11 : 231\n","measurement_5 : 141\n","measurement_6 : 150\n","measurement_7 : 140\n","measurement_4 : 110\n","measurement_15 : 319\n","measurement_10 : 262\n","measurement_16 : 343\n","measurement_14 : 340\n","\n","2638 filled by linear model \n","1706 filled by KNN \n","\n","-------- Product code D ----------\n","\n","filled by linear model :\n","measurement_17 : 398\n","measurement_8 : 146\n","measurement_11 : 265\n","measurement_5 : 87\n","measurement_6 : 118\n","measurement_7 : 146\n","measurement_4 : 88\n","measurement_15 : 313\n","measurement_10 : 174\n","measurement_16 : 322\n","measurement_14 : 316\n","\n","2373 filled by linear model \n","1600 filled by KNN \n","\n","-------- Product code E ----------\n","\n","filled by linear model :\n","measurement_17 : 429\n","measurement_8 : 171\n","measurement_11 : 244\n","measurement_5 : 116\n","measurement_6 : 127\n","measurement_7 : 185\n","measurement_4 : 105\n","measurement_15 : 315\n","measurement_10 : 193\n","measurement_16 : 316\n","measurement_14 : 297\n","\n","2498 filled by linear model \n","1634 filled by KNN \n","\n","-------- Product code F ----------\n","\n","filled by linear model :\n","measurement_17 : 420\n","measurement_8 : 194\n","measurement_11 : 226\n","measurement_5 : 90\n","measurement_6 : 137\n","measurement_7 : 147\n","measurement_4 : 91\n","measurement_15 : 333\n","measurement_10 : 186\n","measurement_16 : 356\n","measurement_14 : 348\n","\n","2528 filled by linear model \n","1545 filled by KNN \n","\n","-------- Product code G ----------\n","\n","filled by linear model :\n","measurement_17 : 373\n","measurement_8 : 188\n","measurement_11 : 221\n","measurement_5 : 104\n","measurement_6 : 146\n","measurement_7 : 145\n","measurement_4 : 93\n","measurement_15 : 299\n","measurement_10 : 226\n","measurement_16 : 343\n","measurement_14 : 268\n","\n","2406 filled by linear model \n","1518 filled by KNN \n","\n","-------- Product code H ----------\n","\n","filled by linear model :\n","measurement_17 : 361\n","measurement_8 : 147\n","measurement_11 : 205\n","measurement_5 : 112\n","measurement_6 : 121\n","measurement_7 : 158\n","measurement_4 : 75\n","measurement_15 : 299\n","measurement_10 : 217\n","measurement_16 : 340\n","measurement_14 : 283\n","\n","2318 filled by linear model \n","1565 filled by KNN \n","\n","-------- Product code I ----------\n","\n","filled by linear model :\n","measurement_17 : 377\n","measurement_8 : 192\n","measurement_11 : 209\n","measurement_5 : 119\n","measurement_6 : 132\n","measurement_7 : 136\n","measurement_4 : 89\n","measurement_15 : 350\n","measurement_10 : 246\n","measurement_16 : 294\n","measurement_14 : 283\n","\n","2427 filled by linear model \n","1402 filled by KNN \n"]}]},{"cell_type":"code","source":["X = ['A', 'B', 'C', 'D', 'E']\n","\n","folds_dict = {}\n","i = 1\n","for j in range(5):\n","  for k in range(j + 1, 5):\n","    tmp_X = X.copy()\n","    tmp_X.remove(X[j])\n","    tmp_X.remove(X[k])\n","    tmpList = list()\n","    tmpList.append(tmp_X)\n","    tmpList.append([X[j], X[k]])\n","    folds_dict['#' + str(i)] = tmpList\n","    i += 1"],"metadata":{"id":"nNLMcF2h2HY7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = joblib.load('model.joblib')\n","test_predictions = np.zeros((df_test.shape[0], 1))\n","\n","for fold in folds_dict.keys():\n","  test_pred = model.predict(df_test[features].values).reshape(-1, 1)\n","  test_predictions += test_pred / 10    "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i3TKBJHe0wMD","executionInfo":{"status":"ok","timestamp":1673066649724,"user_tz":-480,"elapsed":50501,"user":{"displayName":"張可晴","userId":"06130326083553074662"}},"outputId":"47be2cd5-af3e-49af-d7cd-625e7c612250"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["650/650 [==============================] - 3s 5ms/step\n","650/650 [==============================] - 3s 5ms/step\n","650/650 [==============================] - 3s 5ms/step\n","650/650 [==============================] - 3s 5ms/step\n","650/650 [==============================] - 3s 5ms/step\n","650/650 [==============================] - 5s 8ms/step\n","650/650 [==============================] - 5s 8ms/step\n","650/650 [==============================] - 3s 5ms/step\n","650/650 [==============================] - 3s 5ms/step\n","650/650 [==============================] - 3s 5ms/step\n"]}]},{"cell_type":"code","source":["submission['failure'] = test_predictions\n","submission.to_csv('submission.csv', index=False)\n","!cp submission.csv ./gdrive/MyDrive/Final/submission.csv "],"metadata":{"id":"57Ob6INY0yho"},"execution_count":null,"outputs":[]}]}